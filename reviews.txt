> * Your paper can be up to 17 pages, excluding the bibliography as well
>   as the front page (including the extended metadata of the paper,
>   like title, abstract, authors, ...)

CK: So... We need a separate front page where we put the title and
everything? I did not get this from the author instructions... Do you
know what they mean here?

LC: I don't know. Maybe they mean the things you enter on the webpage
when submitting?

CK: I think I'll mail Herman about this tomorrow.

> * If needed, you can use an appendix of at most 5 pages.

CK: That's substantially less than what we need. I would normally say: let's put up a separate version as a technical report with the full appendix. However, if I recall correctly, submission to FSCD happens by submitting first to arxiv. (Although I don't think I've seen instructions yet...) If that is the case, then we cannot also have a separate version with a longer appendix, as Arxiv will complain about two very similar versions of the same paper. What to do with that?

LC: That's LMCS which requires an arxiv version. For FSCD you just
upload the paper to the Dagstuhl Submission Server. I'd say: let's put
up a separate version as a technical report.

CK: Okay. I don't know whether we can still get an arxiv url on time. I propose that we submit something to Arxiv tomorrow, though. With some luck, we'll have a url by Tuesday morning, but I am not sure about that, and Easter might interfere. If that fails, I can ask Herman whether we can submit the version with url a day later, or just link to a page on either my website or yours.

---------- OPEN DISCUSSION POINTS ----------

> *) l29. It is mentioned that polymorphic rewriting frameworks are usually restricted to rank-1 polymorphism, and no references are given either for frameworks with rank-1 polymorphism or for more general frameworks. I think that this statement is incorrect, and that the authors should provide some references like:
>
> For rank-1 polymorphism:
>
> - Polymorphic higher-order recursive path orderings, by J.-P. Jouannaud and A. Rubio, JACM 2007. http://doi.org/10.1145/1206035.1206037
>
> - Normal higher-order termination, by J.-P. Jouannaud and A. Rubio, TOCL 2015. http://doi.org/10.1145/2699913
>
> - Dependent type theory with first-order parameterized data types and well-founded recursion, by David Wahlstedt, PhD, Chalmers University of Technology, 2007.
>
> - Polymorphic Rewrite Rules: Confluence, Type Inference, and Instance Validation, by M. Hamana, best paper award at FLOPS 2018, https://doi.org/10.1007/978-3-319-90686-7_7, web interface on http://www.cs.gunma-u.ac.jp/hamana/polysol/
>
> For full polymorphism:
>
> - Termination of rewriting in the calculus of constructions, D. Walukiewicz-ChrzÄ…szcz, JFP 2003, http://doi.org/10.1017/S0956796802004641
>
> - Definitions by rewriting in the calculus of constructions, F. Blanqui, MSCS 2005, http://doi.org/10.1017/S0960129504004426
>
> - The Dedukti system (https://deducteam.github.io/), developed since many years now, although based on dependent types only can encode full polymorphism by using type-level rewriting.
>
> - Embedding pure type systems in the lambda-Pi-calculus modulo, D. Cousineau and G. Dowek, TLCA 2007, http://doi.org/10.1007/978-3-540-73228-0_9
>
> - Models and Termination of Proof Reduction in the lambda Pi-Calculus Modulo Theory, G. Dowek, ICALP 2017, http://doi.org/10.4230/LIPIcs.ICALP.2017.109
>
> - Multiversal Polymorphic Algebraic Theories: Syntax, Semantics, Translations, and Equational Logic, by Marcelo Fiore and Makoto Hamana, LICS 2013, https://doi.org/10.1109/LICS.2013.59.

CK: Okay... Let's just remove that part of the motivation. :P  It is
truly not that important.  Agreed?

LC: I think we should add some of the references anyway.

CK: But where?

> Finally, note that van de Pol's work has been generalized to other domains than N, in a categorical framework by Makoto Hamana in "An initial algebra approach to term rewriting systems with variable binders", HOSC 2006, 10.1007/s10990-006-8747-5.

CK: The observation here is that we aren't so much extending van de Pol's work, but rather building on the earlier work by Carsten and me that considered higher-order polynomials (based on van de Pol's work). My inclination would be to say:
* those works were monomorphic, now we're going to do full polymorphism
* this goes beyond shallow polymorphism as follows (see example)
* due to this added complexity, we here particularly extend _polynomial_
  interpretations, so not Jaco's full work.

LC: seems reasonable

CK: I have now reformulated the text and added a bit about why we only extend polynomials.
Please check whether you agree (and whether there is still a good place to put in the
given citations).

CK: Note: I also considered, as an alternative, the following structure:
  - first paragraph unaltered
  - second paragraph explains what we do, and that we only expand higher-order polynomials
  - then a \paragraph{About full impredicative polymorphism} or something like that,
    which contains the paragraphs explaining the differences between shallow and full
    polymorphism and the difficulties involved
  - then the last paragraph separately
However, I did not like to put the disclaimer "we don't generalise Jaco's work completely"
on the first page, and wasn't sure about \paragraphs in the introduction, so I left it as
is.  You may feel differently (this alternative setup probably does make it easier to put
the citations in.)

> l47. References [4,5] are not the more recent ones.

LC: we should replace them with some of the other suggested references?

CK: They have been removed altogether. (However, the argument about them not being recent is because the termination method in 2007 paper is incorrect, so I don't like to cite it.) I don't know what to say about those papers on the calculus of constructions or dependent types, so have not put them in.

> l552. [5] is subsumed by more recent publications as mentioned in major remarks and the "The computability path ordering", by F. Blanqui, J.-P. Jouannaud and A. Rubio, LMCS 2015, http://doi.org/10.2168/LMCS-11(4:3)2015.
>

LC: Haven't changed yet, but we should.

CK: We're not citing it anymore now, though.


> - l278. Why would proofs be less perspicuous?

LC: I mentioned formalisation.

CK: I think the reason for the "perspicuous" is something like "an inductive
proof -- and a coinductive proof works in much the same way -- is generally more
practical than reasoning about an arbitrary sequence of terms and types".  But
I don't think that needs much explanation.  Regarding the formalisation: in the
rebuttal you said something on the lines of "a formalisation using this lemma
would basically go via the coinductive definition" -- do you think you could
formulate that in the 0.9 line we have to spare here?


> l139. What is m in m<k?

CK: It's introduced in line 139. Not sure how to make this clearer. Maybe not put it in the definition, since the lay-out may be causing the confusion?

> l92. * is used both as a kind and as an operator between terms and types. This is unfortunate. Couldn't you use different symbols?

CK: Fair point, but I don't know which ones we should then use.

LC: If we want to change this then we should invent a new symbol for
type application. Or omit the application symbols entirely (as is
standard in type theory).

CK: We _usually_ omit the application symbols, but here I think we need
to make them explicit because we say "you can have *this* but not *that*".
I do not know what best to do this on short notice.

> l124, l136, l139, l144, l258: why using parentheses after \forall?

LC: he's right, they're not strictly necessary. Should we change this?

CK: I guess we're not consistent about it, since \forall *is* used without the brackets in Example 3.7 and the text above it. So, while I actually think it increases readability, I wouldn't mind changing this, provided it is done consistently (either always use the brackets or never).

> l161. rewrite rule -> rewriting rule

CK: It's quite standard in the literature to use "rewrite rule". I don't think we need to act on this one.

> l161. Def 3.5. There is no explicit restriction on the kinds of terms that can be used in a set of rewriting rules. Can they be non PFS terms?

CK: No. (Updated.)

> l172. Why \beta is restricted to *?

LC: We really encode system F here, not F-omega. We should reformulate
this somehow. Encoding F-omega would be more tedious, because you then
need a separate application operator for every kind.

CK: Can you do this? You are far more familiar with these systems than I am.


> l352. This is the first time that you mention that your goal is to prove the termination on PFS terms.

LC: Should we mention this earlier?

CK: It should have been clear already, yes. For example, it is implied by the introduction paragraph of Section 3.
Perhaps we should make this more clear, and add an "overview of the paper" paragraph to the introduction?


> l355. Shouldn't C be a PFS context?

LC: contexts were defined only for PFSs. Should we make this somehow
more clear?

CK: I am not sure how.

> l382. Strictly speaking >^J depends on TM too.

LC: fixed

CK: Wait, what changed?


> l424-425. Could you align arrows to make this more readable? Put (*) in the left margin to find it more easily.

LC: fixed?

CK: This seems to be aout 525-525. Aligning arrows would not make this more readable. I did put the (*) in the left margin (or close enough).


> Could you find shorter notations for lift, flatten, etc.? For instance \uparrow for lift, and \downarrow for flatten?

> You could also remove type annotations as they can be inferred from type annotations in abstractions.

LC: should we do this?

CK: At this point, it would mean changing the entire paper. I would rather not do it.


> The only minor issue is that the second example doesn't work out fully. The authors are very clear about that (already in the introduction). I wonder if it is not more clear to present a core calculus + full termination proof (which is already sufficient to prove cut elimination), and leave the extra rules with permutative conversions as an open question for future work.

CK: I would rather not change the paper too far for the final version, so let's not.


> - I couldn't follow the fine details of the definitions and proofs by coinduction. It would do no harm to explain the definition, not only at an intuitive level (although that was helpful as well). Can you give a concrete example on how to derive > or >= already at Def. 4.8?

LC: maybe we should give an example of how to derive > or >=?

CK: I guess we could... How hard would it be to give a coinductive proof here that âˆ€Î±.lift_Î±(1) > âˆ€Î±.lift_Î±(0)?


> - Def 4.2: Would it be possible to _avoid_ introducing the constants flatten, lift, +, * at higher types, and instead DEFINE those as abbreviated terms, with induction on types? (where we see (s*\chi) as smaller than (FORALL x.s). This would avoid quite some complexity, in particular termination of ~~>
> (at least that strategy works for simply typed case)

LC: I don't think it's necessary to explain this in the paper?

CK: I don't think it is.  It wouldn't be bad to have it in a footnote or remark somewhere, but it isn't really necessary either.

> - Is the problem if for interpretation terms s,t "s>t" computable?

LC: should we explain this?

CK: It is an interesting question because if the relation was computable, we would have to justify why we do not provide an algorithm. Is there a one- or two-line explanation why it isn't computable? We could then remark in Section 6, before introducing the lemmas, that since it isn't computable we'll just give some lemmas to work with it.


> - Would it be doable to formalize your proofs in Coq or Agda?

LC: I don't think we need to speculate about this in the paper?

CK: Not necessarily, although formalisation is mentioned as a reason for using a co-inductive definition. We could mention this as a possible direction for future work in section 8, although it might be deceptive since we don't actually intend to do that.

> - l. 172: Please explain the weird type for A. Also, please indicate what A_{x,y} means precisely. I don't see immediately why the rule for A is well-typed, and how the A is used to make the rule for foldl well-typed.

LC: TODO

CK: I wrote something up. Please check!


> - p4, Def 3.2: it would be more clear to present this as a definition of PFS types and terms, and avoid "We assume..." (l. 135)

LC: done

CK: The "done" is only on avoiding "We assume"? (I am not sure what exactly the reviewer wanted here.)


> Once they have the presentation, the authors follow the classic schema for
> proving termination: interpretation definition, orderings, a removal technique
> and an large example. The authors neither go further defining a dependency pair
> notion nor a way to automate the proofs. Since one of the authors have a vast
> experience on adapting dependency pairs to higher-order, it would be really nice
> if she/they could explain the problems they could find to do this task or if it is just a
> straightforward extension.

CK: Should I say more about this? I could add a few lines in the future work where
we already refer to these things, but it would be highly speculative.

---------- COMPLETED BUT PERHAPS USEFUL DISCUSSION FOR THE FUTURE ----------

[DONE] The paper may be sometimes difficult to follow. For instance, in the Definition 4.8 of the ordering on the interpretation set I, which is probably the most delicate and important point of the paper, the authors use co-induction. More explanations on the validity of this definition and how to prove s > t would be welcome: the authors do not discuss the decidability of this ordering, and not everyone is familiar with co-induction.

  CK: Actually there *is* an intuitive explanation right below the definition --
  but those who merely skim or are looking particularly for definitions can easily miss that. I can see two ways to address this:
  1) Put the intuition in a "remark" environment, and in Definition 4.8 add something like (see also Remark 4.9 for the intuition)?
  2) Put the intuition that explains coinduction immediately below line
  258, and put the part from 259 to 263 in a separate definition.
  
  LC: I went with the second suggestion and added some more explanations.
  
  CK: Either way, I think referencing some standard work -- or
  preferably an easily accessible text -- regarding coinduction might be
  interesting, given that some of the likely readers are not that
  well-versed in coinduction.
  
  LC: I added some references. My frustration here has always been that
  I don't know of a single reference that is accessible (in the sense of
  easily understandable) and explains this exactly in the way I want to
  use it and is close to how one normally writes this in e.g. Coq (if
  they accept my ITP paper then maybe there will be one, but I'm very
  unsure about this).


> The authors can perhaps find other interesting polymorphic examples in "Iteration and coiteration schemes for higher-order and nested datatypes", by A. Abel, R. Matthes and T. Uustalu, TCS 2005, http://doi.org/10.1016/j.tcs.2004.10.017.

  CK: Not for a final version, I think. But perhaps good for a journal extension?


[DONE] - In [16], permutative conversions for the simply typed case required exponentiation. This raises two questions: why do you insist on polynomials, why not add 2^x to the basic functions? And: would that help to find an interpretation of the final rules?

  LC: I mentioned that adding exponentiation is not a fundamental problem.

  CK: Might be interesting to see what they do in [16] to manage existential quantification, though. (Although not for the final version of this.)


