> * Your paper can be up to 17 pages, excluding the bibliography as well
>   as the front page (including the extended metadata of the paper,
>   like title, abstract, authors, ...)

CK: So... We need a separate front page where we put the title and
everything? I did not get this from the author instructions... Do you
know what they mean here?

LC: I don't know. Maybe they mean the things you enter on the webpage
when submitting?

CK: I think I'll mail Herman about this tomorrow.

> * If needed, you can use an appendix of at most 5 pages.

CK: That's substantially less than what we need. I would normally say: let's put up a separate version as a technical report with the full appendix. However, if I recall correctly, submission to FSCD happens by submitting first to arxiv. (Although I don't think I've seen instructions yet...) If that is the case, then we cannot also have a separate version with a longer appendix, as Arxiv will complain about two very similar versions of the same paper. What to do with that?

LC: That's LMCS which requires an arxiv version. For FSCD you just
upload the paper to the Dagstuhl Submission Server. I'd say: let's put
up a separate version as a technical report.

CK: Okay. I don't know whether we can still get an arxiv url on time. I propose that we submit something to Arxiv tomorrow, though. With some luck, we'll have a url by Tuesday morning, but I am not sure about that, and Easter might interfere. If that fails, I can ask Herman whether we can submit the version with url a day later, or just link to a page on either my website or yours.

---------- OPEN DISCUSSION POINTS ----------

> *) l29. It is mentioned that polymorphic rewriting frameworks are usually restricted to rank-1 polymorphism, and no references are given either for frameworks with rank-1 polymorphism or for more general frameworks. I think that this statement is incorrect, and that the authors should provide some references like:
>
> For rank-1 polymorphism:
>
> - Polymorphic higher-order recursive path orderings, by J.-P. Jouannaud and A. Rubio, JACM 2007. http://doi.org/10.1145/1206035.1206037

  CK: I do not like to cite it because this paper is incorrect -- particularly in the way it treats types. That's why I tend to cite the 1999 version, which is correct and has been formally proved. Although in all fairness, the other papers listed here might well be incorrect as well; I owuldn't know without reading them thoroughly.

> - Normal higher-order termination, by J.-P. Jouannaud and A. Rubio, TOCL 2015. http://doi.org/10.1145/2699913

  CK: They use a recursive path ordering (and the "normal" is about beta-eta-normal).

> - Dependent type theory with first-order parameterized data types and well-founded recursion, by David Wahlstedt, PhD, Chalmers University of Technology, 2007.

  CK: They use size-change termination, a structural property.

> - Polymorphic Rewrite Rules: Confluence, Type Inference, and Instance Validation, by M. Hamana, best paper award at FLOPS 2018, https://doi.org/10.1007/978-3-319-90686-7_7, web interface on http://www.cs.gunma-u.ac.jp/hamana/polysol/

  CK: This considers confluence, not termination.

> For full polymorphism:
>
> - Termination of rewriting in the calculus of constructions, D. Walukiewicz-ChrzÄ…szcz, JFP 2003, http://doi.org/10.1017/S0956796802004641

  CK: uses an extension of the higher-order recursive path ordering for CoC + rewriting

> - Definitions by rewriting in the calculus of constructions, F. Blanqui, MSCS 2005, http://doi.org/10.1017/S0960129504004426

  CK: uses an extension of the general schema for CoC + rewriting

> - The Dedukti system (https://deducteam.github.io/), developed since many years now, although based on dependent types only can encode full polymorphism by using type-level rewriting.

  CK: considers Lambda-Pi-calculus Modulo, which is lambda-pi-calculus with computation ruless. I cannot find anything about termination.

> - Embedding pure type systems in the lambda-Pi-calculus modulo, D. Cousineau and G. Dowek, TLCA 2007, http://doi.org/10.1007/978-3-540-73228-0_9

  CK: considers Lambda-Pi-calculus Modulo.  Regarding termination, the only thing I could find is http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.102.4096&rep=rep1&type=pdf : proposition 4 (page 9), which reduces it to termination of something else, but does not give a method.

> - Models and Termination of Proof Reduction in the lambda Pi-Calculus Modulo Theory, G. Dowek, ICALP 2017, http://doi.org/10.4230/LIPIcs.ICALP.2017.109

  CK: Termination follows if the system is super-consistent, which means that for all complete \Pi-algebras B the theory has a model in B. This seems somewhat related, as a model is basically an interpretation; however, Pi-algebras are very different from what we do, and they have to prove something for _all_ Pi-algebras while we do it only for one _given_ algebra.

> - Multiversal Polymorphic Algebraic Theories: Syntax, Semantics, Translations, and Equational Logic, by Marcelo Fiore and Makoto Hamana, LICS 2013, https://doi.org/10.1109/LICS.2013.59.

  CK: This seems to define some way of doing polymorphic rewriting? I have no hope of understanding this given that it is steeped in category theory. Termination is not mentioned.


CK: Okay... Let's just remove that part of the motivation. :P  It is
truly not that important.  Agreed?

LC: I think we should add some of the references anyway.

CK: But where?

LC: in the introduction, when mentioning polymorphism. I think we
definitely need to mention related work on termination in polymorphic
systems! You know more about this, so maybe you add. Maybe make a
separate paragraph in the introduction about related work?

CK: But I don't. That's the problem -- I don't know how CoC or the lambda-Pi-calculus relate to what we're doing...

> Finally, note that van de Pol's work has been generalized to other domains than N, in a categorical framework by Makoto Hamana in "An initial algebra approach to term rewriting systems with variable binders", HOSC 2006, 10.1007/s10990-006-8747-5.

CK: The observation here is that we aren't so much extending van de Pol's work, but rather building on the earlier work by Carsten and me that considered higher-order polynomials (based on van de Pol's work). My inclination would be to say:
* those works were monomorphic, now we're going to do full polymorphism
* this goes beyond shallow polymorphism as follows (see example)
* due to this added complexity, we here particularly extend _polynomial_
  interpretations, so not Jaco's full work.

LC: seems reasonable

CK: I have now reformulated the text and added a bit about why we only extend polynomials.
Please check whether you agree (and whether there is still a good place to put in the
given citations).

CK: Note: I also considered, as an alternative, the following structure:
  - first paragraph unaltered
  - second paragraph explains what we do, and that we only expand higher-order polynomials
  - then a \paragraph{About full impredicative polymorphism} or something like that,
    which contains the paragraphs explaining the differences between shallow and full
    polymorphism and the difficulties involved
  - then the last paragraph separately
However, I did not like to put the disclaimer "we don't generalise Jaco's work completely"
on the first page, and wasn't sure about \paragraphs in the introduction, so I left it as
is.  You may feel differently (this alternative setup probably does make it easier to put
the citations in.)

> l47. References [4,5] are not the more recent ones.

LC: we should replace them with some of the other suggested references?

CK: They have been removed altogether. (However, the argument about
them not being recent is because the termination method in 2007 paper
is incorrect, so I don't like to cite it.) I don't know what to say
about those papers on the calculus of constructions or dependent
types, so have not put them in.

LC: in the "calculus of construction" papers they extend the calculus
of constructions (CoC) with rewrite rules, and show that if the
rewrite rules satisfy a specific termination criterion, then the whole
system is terminating. CoC subsumes F-omega, so there is impredicative
polymorphism there. Hence, the termination conditions for the extra
rewrite rules can handle (some) impredicative polymorphism. We should
cite this.

> l552. [5] is subsumed by more recent publications as mentioned in major remarks and the "The computability path ordering", by F. Blanqui, J.-P. Jouannaud and A. Rubio, LMCS 2015, http://doi.org/10.2168/LMCS-11(4:3)2015.
>

LC: Haven't changed yet, but we should.

CK: We're not citing it anymore now, though.


> - l278. Why would proofs be less perspicuous?

LC: I mentioned formalisation.

CK: I think the reason for the "perspicuous" is something like "an inductive
proof -- and a coinductive proof works in much the same way -- is generally more
practical than reasoning about an arbitrary sequence of terms and types".  But
I don't think that needs much explanation.  Regarding the formalisation: in the
rebuttal you said something on the lines of "a formalisation using this lemma
would basically go via the coinductive definition" -- do you think you could
formulate that in the 0.9 line we have to spare here?

LC: wrote something

> l139. What is m in m<k?

CK: It's introduced in line 139. Not sure how to make this clearer. Maybe not put it in the definition, since the lay-out may be causing the confusion?

> l92. * is used both as a kind and as an operator between terms and types. This is unfortunate. Couldn't you use different symbols?

CK: Fair point, but I don't know which ones we should then use.

LC: If we want to change this then we should invent a new symbol for
type application. Or omit the application symbols entirely (as is
standard in type theory).

CK: We _usually_ omit the application symbols, but here I think we need
to make them explicit because we say "you can have *this* but not *that*".
I do not know what best to do this on short notice.

LC: let's leave it

> l124, l136, l139, l144, l258: why using parentheses after \forall?

LC: he's right, they're not strictly necessary. Should we change this?

CK: I guess we're not consistent about it, since \forall *is* used
without the brackets in Example 3.7 and the text above it. So, while I
actually think it increases readability, I wouldn't mind changing
this, provided it is done consistently (either always use the brackets
or never).

LC: leave it, I can find all forall's, sometimes we use \quant,
sometimes \forall

> l161. rewrite rule -> rewriting rule

CK: It's quite standard in the literature to use "rewrite rule". I don't think we need to act on this one.

> l161. Def 3.5. There is no explicit restriction on the kinds of terms that can be used in a set of rewriting rules. Can they be non PFS terms?

CK: No. (Updated.)

> l172. Why \beta is restricted to *?

LC: We really encode system F here, not F-omega. We should reformulate
this somehow. Encoding F-omega would be more tedious, because you then
need a separate application operator for every kind.

CK: Can you do this? You are far more familiar with these systems than I am.

LC: OK, I just said we're encoding system F instead of F-omega

> l352. This is the first time that you mention that your goal is to prove the termination on PFS terms.

LC: Should we mention this earlier?

CK: It should have been clear already, yes. For example, it is implied by the introduction paragraph of Section 3.
Perhaps we should make this more clear, and add an "overview of the paper" paragraph to the introduction?

LC: I never like the "overview" paragraphs in introductions. They usually
say what the names of the sections are and duplicate their first
sentences. If the paper is not too long, you can just look at the
section names, so I think it makes no sense to duplicate this
information.

> l355. Shouldn't C be a PFS context?

LC: contexts were defined only for PFSs. Should we make this somehow
more clear?

CK: I am not sure how.

> l382. Strictly speaking >^J depends on TM too.

LC: fixed

CK: Wait, what changed?

LC: I explicitly remarked that there is a dependency, without changing
the notation.

> l424-425. Could you align arrows to make this more readable? Put (*) in the left margin to find it more easily.

LC: fixed?

CK: This seems to be aout 525-525. Aligning arrows would not make this more readable. I did put the (*) in the left margin (or close enough).


> Could you find shorter notations for lift, flatten, etc.? For instance \uparrow for lift, and \downarrow for flatten?

> You could also remove type annotations as they can be inferred from type annotations in abstractions.

LC: should we do this?

CK: At this point, it would mean changing the entire paper. I would rather not do it.

LC: ok, let's leave it

> The only minor issue is that the second example doesn't work out fully. The authors are very clear about that (already in the introduction). I wonder if it is not more clear to present a core calculus + full termination proof (which is already sufficient to prove cut elimination), and leave the extra rules with permutative conversions as an open question for future work.

CK: I would rather not change the paper too far for the final version, so let's not.

LC: I think the comment is stupid, because termination of the core
calculus can be shown by a completely standard encoding in system F,
whose termination is well-known. These are the permutative conversions
which are of any difficulty here, and the point is to show that the
method can handle at least some of them. In fact, because the extra
connectives may be encoded in F, it makes no sense to even consider
the core calculus without the permutative conversions (when you don't
need permutative conversions, just use system F with other connectives
encoded).

> - I couldn't follow the fine details of the definitions and proofs by coinduction. It would do no harm to explain the definition, not only at an intuitive level (although that was helpful as well). Can you give a concrete example on how to derive > or >= already at Def. 4.8?

LC: maybe we should give an example of how to derive > or >=?

CK: I guess we could... How hard would it be to give a coinductive proof here that âˆ€Î±.lift_Î±(1) > âˆ€Î±.lift_Î±(0)?

LC: You meant \Lambda Î± . lift_Î±(1) > \Lambda Î± . lift_Î±(0)? This can
only be shown by generalising to lift_sigma(1) > lift_sigma(0) for any
closed type sigma, and we already have Lemma lem:liftgreater for this.

> - Def 4.2: Would it be possible to _avoid_ introducing the constants flatten, lift, +, * at higher types, and instead DEFINE those as abbreviated terms, with induction on types? (where we see (s*\chi) as smaller than (FORALL x.s). This would avoid quite some complexity, in particular termination of ~~>
> (at least that strategy works for simply typed case)

LC: I don't think it's necessary to explain this in the paper?

CK: I don't think it is.  It wouldn't be bad to have it in a footnote or remark somewhere, but it isn't really necessary either.

> - Is the problem if for interpretation terms s,t "s>t" computable?

LC: should we explain this?

CK: It is an interesting question because if the relation was computable, we would have to justify why we do not provide an algorithm. Is there a one- or two-line explanation why it isn't computable? We could then remark in Section 6, before introducing the lemmas, that since it isn't computable we'll just give some lemmas to work with it.

LC: not sure about the explanation; it's infinite, so there's no
obvious way to compute it, but I don't think I have a precise argument.

> - Would it be doable to formalize your proofs in Coq or Agda?

LC: I don't think we need to speculate about this in the paper?

CK: Not necessarily, although formalisation is mentioned as a reason for using a co-inductive definition. We could mention this as a possible direction for future work in section 8, although it might be deceptive since we don't actually intend to do that.

> - l. 172: Please explain the weird type for A. Also, please indicate what A_{x,y} means precisely. I don't see immediately why the rule for A is well-typed, and how the A is used to make the rule for foldl well-typed.

LC: TODO

CK: I wrote something up. Please check!

LC: looks good

> - p4, Def 3.2: it would be more clear to present this as a definition of PFS types and terms, and avoid "We assume..." (l. 135)

LC: done

CK: The "done" is only on avoiding "We assume"? (I am not sure what exactly the reviewer wanted here.)

LC: yes, I'm also actually not completely sure either, just rephrased the
definition slightly

> Once they have the presentation, the authors follow the classic schema for
> proving termination: interpretation definition, orderings, a removal technique
> and an large example. The authors neither go further defining a dependency pair
> notion nor a way to automate the proofs. Since one of the authors have a vast
> experience on adapting dependency pairs to higher-order, it would be really nice
> if she/they could explain the problems they could find to do this task or if it is just a
> straightforward extension.

CK: Should I say more about this? I could add a few lines in the future work where
we already refer to these things, but it would be highly speculative.

LC: I think there's enough.

---------- COMPLETED BUT PERHAPS USEFUL DISCUSSION FOR THE FUTURE ----------

[DONE] The paper may be sometimes difficult to follow. For instance, in the Definition 4.8 of the ordering on the interpretation set I, which is probably the most delicate and important point of the paper, the authors use co-induction. More explanations on the validity of this definition and how to prove s > t would be welcome: the authors do not discuss the decidability of this ordering, and not everyone is familiar with co-induction.

  CK: Actually there *is* an intuitive explanation right below the definition --
  but those who merely skim or are looking particularly for definitions can easily miss that. I can see two ways to address this:
  1) Put the intuition in a "remark" environment, and in Definition 4.8 add something like (see also Remark 4.9 for the intuition)?
  2) Put the intuition that explains coinduction immediately below line
  258, and put the part from 259 to 263 in a separate definition.

  LC: I went with the second suggestion and added some more explanations.

  CK: Either way, I think referencing some standard work -- or
  preferably an easily accessible text -- regarding coinduction might be
  interesting, given that some of the likely readers are not that
  well-versed in coinduction.

  LC: I added some references. My frustration here has always been that
  I don't know of a single reference that is accessible (in the sense of
  easily understandable) and explains this exactly in the way I want to
  use it and is close to how one normally writes this in e.g. Coq (if
  they accept my ITP paper then maybe there will be one, but I'm very
  unsure about this).


> The authors can perhaps find other interesting polymorphic examples in "Iteration and coiteration schemes for higher-order and nested datatypes", by A. Abel, R. Matthes and T. Uustalu, TCS 2005, http://doi.org/10.1016/j.tcs.2004.10.017.

  CK: Not for a final version, I think. But perhaps good for a journal extension?


[DONE] - In [16], permutative conversions for the simply typed case required exponentiation. This raises two questions: why do you insist on polynomials, why not add 2^x to the basic functions? And: would that help to find an interpretation of the final rules?

  LC: I mentioned that adding exponentiation is not a fundamental problem.

  CK: Might be interesting to see what they do in [16] to manage existential quantification, though. (Although not for the final version of this.)

LC: they erase to conjunction, but this won't work here; there they
have a first-order quantifier which is not impredicative, so you can
"erase" it. This is, by the way, a general method that usually works
with non-polymorphic dependent types (first-order individual
quantification is one) -- you just forget the dependencies -- in this
way you can e.g. reduce the termination of the calculus of
constructions to termination of F-omega -- when you erase the
non-polymorphic dependencies you're left with the "core" polymorphic
calculus. That's why I suggested concentrating on polymorphism when we
were talking just about doing proofs for some systems with permutative
conversions, because impredicative polymorphism seems to be the hard
part.
